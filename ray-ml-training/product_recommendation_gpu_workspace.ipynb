{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Product Recommendation Model with GPU Training (Workspaces)\n",
    "\n",
    "This notebook demonstrates GPU-accelerated ML model training in **Snowflake Notebooks in Workspaces**.\n",
    "\n",
    "**Use Case:** Predict which products a customer is likely to purchase next based on their order history\n",
    "\n",
    "**What we'll do:**\n",
    "1. Set execution context for Workspaces\n",
    "2. Load customer order data from Snowflake\n",
    "3. Engineer features from purchase patterns\n",
    "4. Train XGBoost model with GPU acceleration\n",
    "5. Evaluate model performance\n",
    "6. Save model to Snowflake Model Registry\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run in Snowflake Notebooks in Workspaces with GPU compute pool\n",
    "- Data in AUTOMATED_INTELLIGENCE.RAW.ORDERS and ORDER_ITEMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-context",
   "metadata": {},
   "source": [
    "## 1. Set Execution Context\n",
    "\n",
    "Notebooks in Workspaces don't automatically set database or schema. We need to explicitly set the execution context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "set-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Set execution context for Workspaces\n",
    "USE DATABASE AUTOMATED_INTELLIGENCE;\n",
    "USE SCHEMA RAW;\n",
    "USE WAREHOUSE COMPUTE_WH;  -- Adjust to your warehouse name\n",
    "\n",
    "-- Verify context\n",
    "SELECT CURRENT_DATABASE() as database, \n",
    "       CURRENT_SCHEMA() as schema, \n",
    "       CURRENT_WAREHOUSE() as warehouse,\n",
    "       CURRENT_ROLE() as role;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-gpu",
   "metadata": {},
   "source": [
    "## 2. Check GPU Availability\n",
    "\n",
    "Verify that GPU is available in the compute pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Check GPU with nvidia-smi\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "        capture_output=True, \n",
    "        text=True, \n",
    "        timeout=5\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        gpu_info = result.stdout.strip()\n",
    "        print(f\"ðŸš€ GPU Detected: {gpu_info}\")\n",
    "        GPU_AVAILABLE = True\n",
    "    else:\n",
    "        print(\"âš ï¸  No GPU detected - will use CPU training\")\n",
    "        GPU_AVAILABLE = False\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not detect GPU: {e}\")\n",
    "    print(\"   Will use CPU training\")\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "print(f\"\\n{'GPU' if GPU_AVAILABLE else 'CPU'} training will be used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 3. Import Libraries\n",
    "\n",
    "Import required libraries for data processing, ML, and Snowflake integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"   XGBoost version: {xgb.__version__}\")\n",
    "print(f\"   Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data\n",
    "\n",
    "Query orders and order items using fully qualified table names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-orders",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Check data volume with fully qualified names\n",
    "SELECT \n",
    "    'ORDERS' as table_name,\n",
    "    COUNT(*) as row_count,\n",
    "    COUNT(DISTINCT CUSTOMER_ID) as unique_customers\n",
    "FROM AUTOMATED_INTELLIGENCE.RAW.ORDERS\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'ORDER_ITEMS' as table_name,\n",
    "    COUNT(*) as row_count,\n",
    "    COUNT(DISTINCT ORDER_ID) as unique_orders\n",
    "FROM AUTOMATED_INTELLIGENCE.RAW.ORDER_ITEMS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-orders",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Sample orders\n",
    "SELECT \n",
    "    ORDER_ID, \n",
    "    CUSTOMER_ID, \n",
    "    ORDER_DATE, \n",
    "    TOTAL_AMOUNT, \n",
    "    ORDER_STATUS\n",
    "FROM AUTOMATED_INTELLIGENCE.RAW.ORDERS\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-items",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Sample order items\n",
    "SELECT \n",
    "    ORDER_ID, \n",
    "    PRODUCT_ID, \n",
    "    QUANTITY, \n",
    "    UNIT_PRICE, \n",
    "    LINE_TOTAL\n",
    "FROM AUTOMATED_INTELLIGENCE.RAW.ORDER_ITEMS\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Create customer and product features for recommendation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Create training dataset with customer-product features\n",
    "CREATE OR REPLACE TEMPORARY TABLE training_features AS\n",
    "WITH customer_stats AS (\n",
    "    SELECT \n",
    "        o.CUSTOMER_ID,\n",
    "        COUNT(DISTINCT o.ORDER_ID) as TOTAL_PURCHASES,\n",
    "        SUM(oi.LINE_TOTAL) as TOTAL_SPENT,\n",
    "        AVG(oi.LINE_TOTAL) as AVG_ITEM_SPEND,\n",
    "        COUNT(DISTINCT oi.PRODUCT_ID) as UNIQUE_PRODUCTS,\n",
    "        MAX(o.ORDER_DATE) as LAST_PURCHASE_DATE,\n",
    "        DATEDIFF(day, MAX(o.ORDER_DATE), CURRENT_DATE()) as DAYS_SINCE_LAST_PURCHASE\n",
    "    FROM AUTOMATED_INTELLIGENCE.RAW.ORDERS o\n",
    "    JOIN AUTOMATED_INTELLIGENCE.RAW.ORDER_ITEMS oi \n",
    "        ON o.ORDER_ID = oi.ORDER_ID\n",
    "    GROUP BY o.CUSTOMER_ID\n",
    "),\n",
    "customer_products AS (\n",
    "    SELECT \n",
    "        o.CUSTOMER_ID,\n",
    "        oi.PRODUCT_ID,\n",
    "        COUNT(*) as PURCHASE_COUNT,\n",
    "        SUM(oi.QUANTITY) as TOTAL_QUANTITY,\n",
    "        MAX(o.ORDER_DATE) as LAST_PRODUCT_PURCHASE\n",
    "    FROM AUTOMATED_INTELLIGENCE.RAW.ORDERS o\n",
    "    JOIN AUTOMATED_INTELLIGENCE.RAW.ORDER_ITEMS oi \n",
    "        ON o.ORDER_ID = oi.ORDER_ID\n",
    "    GROUP BY o.CUSTOMER_ID, oi.PRODUCT_ID\n",
    ")\n",
    "SELECT \n",
    "    cp.CUSTOMER_ID,\n",
    "    cp.PRODUCT_ID,\n",
    "    cs.TOTAL_PURCHASES,\n",
    "    cs.TOTAL_SPENT,\n",
    "    cs.AVG_ITEM_SPEND,\n",
    "    cs.UNIQUE_PRODUCTS,\n",
    "    cs.DAYS_SINCE_LAST_PURCHASE,\n",
    "    cp.PURCHASE_COUNT,\n",
    "    cp.TOTAL_QUANTITY,\n",
    "    1 as PURCHASED  -- Label: customer purchased this product\n",
    "FROM customer_products cp\n",
    "JOIN customer_stats cs ON cp.CUSTOMER_ID = cs.CUSTOMER_ID\n",
    "-- Sample for faster training (adjust as needed)\n",
    "SAMPLE (100000 ROWS);\n",
    "\n",
    "-- Check results\n",
    "SELECT COUNT(*) as training_examples FROM training_features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-features-pandas",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Preview features\n",
    "SELECT * FROM training_features LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-training",
   "metadata": {},
   "source": [
    "## 6. Load Data into Pandas for Training\n",
    "\n",
    "Reference the SQL result using cell referencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-to-pandas",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Load all training data\n",
    "SELECT * FROM training_features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference SQL cell result (assuming previous cell is dataframe_1)\n",
    "# Note: Adjust dataframe_X number based on actual cell execution order\n",
    "pdf = dataframe_1.copy()  # Replace with actual dataframe number\n",
    "\n",
    "print(f\"âœ… Loaded {len(pdf):,} training examples\")\n",
    "print(f\"\\nðŸ“Š Data Shape: {pdf.shape}\")\n",
    "print(f\"\\nðŸ“ˆ Feature Summary:\")\n",
    "print(pdf.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encode-features",
   "metadata": {},
   "source": [
    "## 7. Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode customer_id and product_id\n",
    "customer_encoder = LabelEncoder()\n",
    "product_encoder = LabelEncoder()\n",
    "\n",
    "pdf['CUSTOMER_ID_ENCODED'] = customer_encoder.fit_transform(pdf['CUSTOMER_ID'])\n",
    "pdf['PRODUCT_ID_ENCODED'] = product_encoder.fit_transform(pdf['PRODUCT_ID'])\n",
    "\n",
    "# Define features for training\n",
    "feature_columns = [\n",
    "    'CUSTOMER_ID_ENCODED',\n",
    "    'PRODUCT_ID_ENCODED',\n",
    "    'TOTAL_PURCHASES',\n",
    "    'TOTAL_SPENT',\n",
    "    'AVG_ITEM_SPEND',\n",
    "    'UNIQUE_PRODUCTS',\n",
    "    'DAYS_SINCE_LAST_PURCHASE',\n",
    "    'PURCHASE_COUNT',\n",
    "    'TOTAL_QUANTITY'\n",
    "]\n",
    "\n",
    "label_column = 'PURCHASED'\n",
    "\n",
    "# Prepare X and y\n",
    "X = pdf[feature_columns]\n",
    "y = pdf[label_column]\n",
    "\n",
    "print(f\"ðŸ“Š Features shape: {X.shape}\")\n",
    "print(f\"   Label distribution: {y.value_counts().to_dict()}\")\n",
    "print(f\"\\nâœ… Features encoded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-model",
   "metadata": {},
   "source": [
    "## 8. Train XGBoost Model with GPU Acceleration\n",
    "\n",
    "**GPU Training Configuration:**\n",
    "- `tree_method='gpu_hist'`: Use GPU-accelerated histogram-based training\n",
    "- `predictor='gpu_predictor'`: Use GPU for predictions\n",
    "- Significantly faster than CPU for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Data Split:\")\n",
    "print(f\"   Training set: {len(X_train):,} samples\")\n",
    "print(f\"   Test set: {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-xgboost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Configure XGBoost for GPU training\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"ðŸš€ Training with GPU acceleration...\")\n",
    "    xgb_params = {\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.1,\n",
    "        'tree_method': 'gpu_hist',  # GPU-accelerated training\n",
    "        'predictor': 'gpu_predictor',  # GPU-accelerated prediction\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "else:\n",
    "    print(\"ðŸ’» Training with CPU...\")\n",
    "    xgb_params = {\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.1,\n",
    "        'tree_method': 'hist',  # CPU histogram-based training\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'logloss',\n",
    "        'n_jobs': -1  # Use all CPU cores\n",
    "    }\n",
    "\n",
    "# Create and train model\n",
    "model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "print(f\"\\nâ³ Training started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=50  # Print progress every 50 iterations\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"   Training time: {training_time:.2f} seconds\")\n",
    "print(f\"   Model: XGBoost Classifier\")\n",
    "print(f\"   Features: {len(feature_columns)}\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"ðŸ“Š Model Performance:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Purchased', 'Purchased']))\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Summary Metrics:\")\n",
    "print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall: {recall:.4f}\")\n",
    "print(f\"   F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Business interpretation\n",
    "print(f\"\\nðŸ’¼ Business Impact:\")\n",
    "print(f\"   If we recommend products to customers:\")\n",
    "print(f\"   - {precision*100:.1f}% of recommendations will be relevant\")\n",
    "print(f\"   - We'll capture {recall*100:.1f}% of products customers want\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"ðŸ” Feature Importance:\")\n",
    "print(feature_importance_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "top_3_features = feature_importance_df.head(3)\n",
    "for idx, row in top_3_features.iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-model",
   "metadata": {},
   "source": [
    "## 11. Save Model to Snowflake Model Registry\n",
    "\n",
    "**Note:** Requires Snowflake ML package and appropriate permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registry-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Snowflake ML registry\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Create Snowpark session for Workspaces\n",
    "# In Workspaces, connection is handled by the service\n",
    "import snowflake.connector\n",
    "import os\n",
    "\n",
    "# Get connection from environment (Workspaces provides this)\n",
    "conn = snowflake.connector.connect(\n",
    "    account=os.environ.get('SNOWFLAKE_ACCOUNT'),\n",
    "    user=os.environ.get('SNOWFLAKE_USER'),\n",
    "    authenticator='externalbrowser'  # Use SSO if available\n",
    ")\n",
    "\n",
    "# Create Snowpark session\n",
    "from snowflake.snowpark import Session\n",
    "session = Session.builder.configs({'connection': conn}).create()\n",
    "\n",
    "print(\"âœ… Snowpark session created\")\n",
    "print(f\"   Current database: {session.get_current_database()}\")\n",
    "print(f\"   Current schema: {session.get_current_schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-to-registry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create registry\n",
    "registry = Registry(\n",
    "    session=session,\n",
    "    database_name=\"AUTOMATED_INTELLIGENCE\",\n",
    "    schema_name=\"MODELS\"\n",
    ")\n",
    "\n",
    "# Prepare model metadata\n",
    "model_name = \"product_recommendation_xgboost\"\n",
    "version_name = f\"v_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Sample input for schema inference\n",
    "sample_input = X_test.head(5)\n",
    "\n",
    "# Save model\n",
    "print(f\"ðŸ’¾ Saving model to registry...\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Version: {version_name}\")\n",
    "\n",
    "try:\n",
    "    mv = registry.log_model(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        version_name=version_name,\n",
    "        sample_input_data=sample_input,\n",
    "        comment=f\"XGBoost product recommender {'(GPU-trained)' if GPU_AVAILABLE else '(CPU-trained)'}. Trained on {len(X_train):,} customer-product pairs. F1: {f1:.4f}\",\n",
    "        metrics={\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1_score\": float(f1),\n",
    "            \"training_samples\": int(len(X_train)),\n",
    "            \"test_samples\": int(len(X_test)),\n",
    "            \"training_time_seconds\": float(training_time),\n",
    "            \"gpu_accelerated\": GPU_AVAILABLE\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Model saved successfully!\")\n",
    "    print(f\"\\nðŸ“¦ Model Details:\")\n",
    "    print(f\"   Database: AUTOMATED_INTELLIGENCE\")\n",
    "    print(f\"   Schema: MODELS\")\n",
    "    print(f\"   Model: {model_name}\")\n",
    "    print(f\"   Version: {version_name}\")\n",
    "    print(f\"   GPU Accelerated: {GPU_AVAILABLE}\")\n",
    "    print(f\"   Training Time: {training_time:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Error saving model: {e}\")\n",
    "    print(\"   Check that AUTOMATED_INTELLIGENCE.MODELS schema exists and you have write permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-predictions",
   "metadata": {},
   "source": [
    "## 12. Test Model Predictions\n",
    "\n",
    "Generate product recommendations for sample customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for test set\n",
    "X_sample = X_test.head(20)\n",
    "predictions_proba = model.predict_proba(X_sample)[:, 1]\n",
    "\n",
    "# Create results DataFrame\n",
    "results = X_sample.copy()\n",
    "results['PURCHASE_PROBABILITY'] = predictions_proba\n",
    "results['PREDICTION'] = (predictions_proba > 0.5).astype(int)\n",
    "results['ACTUAL'] = y_test.iloc[:20].values\n",
    "\n",
    "# Sort by probability\n",
    "results = results.sort_values('PURCHASE_PROBABILITY', ascending=False)\n",
    "\n",
    "print(\"ðŸ”® Top Product Recommendations:\")\n",
    "print(results[['CUSTOMER_ID_ENCODED', 'PRODUCT_ID_ENCODED', \n",
    "               'PURCHASE_PROBABILITY', 'PREDICTION', 'ACTUAL']].to_string(index=False))\n",
    "\n",
    "# Calculate recommendation accuracy\n",
    "correct = (results['PREDICTION'] == results['ACTUAL']).sum()\n",
    "total = len(results)\n",
    "print(f\"\\nâœ… Recommendation Accuracy: {correct}/{total} ({correct/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "- âœ… Set execution context for Notebooks in Workspaces\n",
    "- âœ… Loaded customer order data using fully qualified table names\n",
    "- âœ… Engineered features from purchase patterns\n",
    "- âœ… Trained XGBoost model with GPU acceleration (if available)\n",
    "- âœ… Achieved good prediction accuracy for product recommendations\n",
    "- âœ… Saved model to Snowflake Model Registry\n",
    "\n",
    "**Key Differences in Workspaces:**\n",
    "- Explicit database/schema setting with `USE` statements\n",
    "- Fully qualified table names for portability\n",
    "- Service-based compute with long-running kernels\n",
    "- Terminal access for package management\n",
    "- IPython kernel with Jupyter magics support\n",
    "\n",
    "**GPU Benefits:**\n",
    "- Faster training on large datasets\n",
    "- Can handle more trees and deeper models\n",
    "- Same prediction accuracy as CPU\n",
    "\n",
    "**Next Steps:**\n",
    "1. Deploy model for real-time recommendations\n",
    "2. A/B test recommendations in production\n",
    "3. Monitor model performance over time\n",
    "4. Retrain periodically with new data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
