{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Product Recommendation Model with GPU Training (Workspaces)\n\nThis notebook demonstrates GPU-accelerated ML model training in **Snowflake Notebooks in Workspaces**.\n\n**Use Case:** Predict which products a customer is likely to purchase next based on their order history\n\n**What we'll do:**\n1. Set execution context for Workspaces\n2. Load customer order data from Snowflake\n3. Engineer features from purchase patterns\n4. Train XGBoost model with GPU acceleration\n5. Evaluate model performance\n6. Save model to Snowflake Model Registry\n\n**Prerequisites:**\n- Run in Snowflake Notebooks in Workspaces with GPU compute pool\n- Data in AUTOMATED_INTELLIGENCE.RAW.ORDERS and ORDER_ITEMS"
    },
    {
      "cell_type": "markdown",
      "id": "setup-context",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Set Execution Context\n\nNotebooks in Workspaces don't automatically set database or schema. We need to explicitly set the execution context."
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "import-libs",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Standard libraries\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport time\nimport warnings\n \n# Snowpark for session management and data processing\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import (\n    col, count, count_distinct, sum as sum_, avg, max as max_, min as min_,\n    datediff, current_date, stddev, coalesce, lit, when\n)\n\n# Snowflake ML Registry\nfrom snowflake.ml.registry import Registry\n\n# Machine Learning libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import (\n    classification_report, \n    accuracy_score, \n    precision_score, \n    recall_score, \n    f1_score\n)\nimport xgboost as xgb\n\nwarnings.filterwarnings('ignore')\n\n# Get active Snowpark session\nsession = get_active_session()\n\nprint(\"âœ… Libraries imported successfully\")\nprint(f\"   XGBoost version: {xgb.__version__}\")\nprint(f\"   Pandas version: {pd.__version__}\")\nprint(f\"\\nðŸš€ Ready for GPU-accelerated training!\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "set-context",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Set execution context using Snowpark\nsession.use_database('AUTOMATED_INTELLIGENCE')\nsession.use_schema('RAW')\nsession.use_warehouse('COMPUTE_WH')  # Adjust to your warehouse name\n\n# Verify context\ncontext_df = session.sql(\"\"\"\n    SELECT CURRENT_DATABASE() as database, \n           CURRENT_SCHEMA() as schema, \n           CURRENT_WAREHOUSE() as warehouse,\n           CURRENT_ROLE() as role\n\"\"\")\n\nprint(\"âœ… Execution context set:\")\ncontext_df.show()"
    },
    {
      "cell_type": "markdown",
      "id": "load-data",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Load and Explore Data with Snowpark\n\nQuery orders and order items using Snowpark DataFrames with fully qualified table names."
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "query-orders",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Check data volume using Snowpark\nprint(\"ðŸ“Š Checking data volume...\\n\")\n\n# Query orders table\norders_df = session.table('AUTOMATED_INTELLIGENCE.RAW.ORDERS')\norders_stats = orders_df.select(\n    lit('ORDERS').alias('TABLE_NAME'),\n    count('*').alias('ROW_COUNT'),\n    count_distinct('CUSTOMER_ID').alias('UNIQUE_CUSTOMERS')\n)\n\n# Query order items table\norder_items_df = session.table('AUTOMATED_INTELLIGENCE.RAW.ORDER_ITEMS')\nitems_stats = order_items_df.select(\n    lit('ORDER_ITEMS').alias('TABLE_NAME'),\n    count('*').alias('ROW_COUNT'),\n    count_distinct('ORDER_ID').alias('UNIQUE_ORDERS')\n)\n\n# Union and display\ndata_stats = orders_stats.union(items_stats)\nprint(\"Data Volume:\")\ndata_stats.show()"
    },
    {
      "cell_type": "markdown",
      "id": "feature-engineering",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Feature Engineering\n\nCreate customer and product features for recommendation model."
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "47966816",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Step 1: Create customer statistics using Snowpark\nprint(\"ðŸ“Š Step 1: Creating customer statistics...\")\n\n# Read orders and order items\norders = session.table('AUTOMATED_INTELLIGENCE.RAW.ORDERS')\norder_items = session.table('AUTOMATED_INTELLIGENCE.RAW.ORDER_ITEMS')\n\n# Join and aggregate using explicit DataFrame references to avoid ambiguity\ncustomer_stats = orders.join(\n    order_items, \n    orders['ORDER_ID'] == order_items['ORDER_ID']\n).group_by(orders['CUSTOMER_ID']).agg(\n    count_distinct(orders['ORDER_ID']).alias('TOTAL_PURCHASES'),\n    sum_(order_items['LINE_TOTAL']).alias('TOTAL_SPENT'),\n    avg(order_items['LINE_TOTAL']).alias('AVG_ITEM_SPEND'),\n    count_distinct(order_items['PRODUCT_ID']).alias('UNIQUE_PRODUCTS'),\n    max_(orders['ORDER_DATE']).alias('LAST_PURCHASE_DATE'),\n    datediff('day', max_(orders['ORDER_DATE']), current_date()).alias('DAYS_SINCE_LAST_PURCHASE')\n)\n\n# Save as temp table\ncustomer_stats.write.mode('overwrite').save_as_table('customer_stats', table_type='temporary')\n\nprint(f\"âœ… Created customer_stats table with {customer_stats.count():,} customers\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "727d05c0",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Step 2: Create customer-product pairs using Snowpark\nprint(\"ðŸ“Š Step 2: Creating customer-product purchase history...\")\n\n# Join and aggregate customer-product pairs using explicit DataFrame references\ncustomer_products = orders.join(\n    order_items,\n    orders['ORDER_ID'] == order_items['ORDER_ID']\n).group_by([orders['CUSTOMER_ID'], order_items['PRODUCT_ID']]).agg(\n    count(lit(1)).alias('PURCHASE_COUNT'),\n    sum_(order_items['QUANTITY']).alias('TOTAL_QUANTITY'),\n    max_(orders['ORDER_DATE']).alias('LAST_PRODUCT_PURCHASE')\n)\n\n# Save as temp table\ncustomer_products.write.mode('overwrite').save_as_table('customer_products', table_type='temporary')\n\nprint(f\"âœ… Created customer_products table with {customer_products.count():,} customer-product pairs\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6048dea7",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%sql\n-- Step 3: Create large training dataset to maximize GPU utilization\n-- Generates ~5M training samples with rich features\nCREATE OR REPLACE TEMPORARY TABLE training_features AS\nWITH customer_history AS (\n    SELECT \n        o.CUSTOMER_ID,\n        COUNT(DISTINCT o.ORDER_ID) as TOTAL_PAST_ORDERS,\n        SUM(oi.LINE_TOTAL) as TOTAL_SPENT,\n        AVG(oi.LINE_TOTAL) as AVG_ITEM_SPEND,\n        COUNT(DISTINCT oi.PRODUCT_ID) as UNIQUE_PRODUCTS_BOUGHT,\n        DATEDIFF(day, MAX(o.ORDER_DATE), CURRENT_DATE()) as DAYS_SINCE_LAST_ORDER,\n        LISTAGG(DISTINCT oi.PRODUCT_ID, ',') as PURCHASED_PRODUCTS\n    FROM AUTOMATED_INTELLIGENCE.RAW.ORDERS o\n    JOIN AUTOMATED_INTELLIGENCE.RAW.ORDER_ITEMS oi ON o.ORDER_ID = oi.ORDER_ID\n    GROUP BY o.CUSTOMER_ID\n),\nproduct_popularity AS (\n    SELECT \n        PRODUCT_ID,\n        COUNT(DISTINCT ORDER_ID) as TIMES_ORDERED,\n        AVG(UNIT_PRICE) as AVG_PRICE,\n        SUM(QUANTITY) as TOTAL_QUANTITY_SOLD,\n        STDDEV(UNIT_PRICE) as PRICE_VARIANCE\n    FROM AUTOMATED_INTELLIGENCE.RAW.ORDER_ITEMS\n    GROUP BY PRODUCT_ID\n),\npositive_examples AS (\n    SELECT \n        cp.CUSTOMER_ID,\n        cp.PRODUCT_ID,\n        ch.TOTAL_PAST_ORDERS,\n        ch.TOTAL_SPENT,\n        ch.AVG_ITEM_SPEND,\n        ch.UNIQUE_PRODUCTS_BOUGHT,\n        ch.DAYS_SINCE_LAST_ORDER,\n        pp.TIMES_ORDERED as PRODUCT_POPULARITY,\n        pp.AVG_PRICE as PRODUCT_PRICE,\n        pp.TOTAL_QUANTITY_SOLD as PRODUCT_VOLUME,\n        COALESCE(pp.PRICE_VARIANCE, 0) as PRODUCT_PRICE_VARIANCE,\n        1 as PURCHASED\n    FROM customer_products cp\n    JOIN customer_history ch ON cp.CUSTOMER_ID = ch.CUSTOMER_ID\n    JOIN product_popularity pp ON cp.PRODUCT_ID = pp.PRODUCT_ID\n),\nnegative_examples AS (\n    SELECT \n        ch.CUSTOMER_ID,\n        pp.PRODUCT_ID,\n        ch.TOTAL_PAST_ORDERS,\n        ch.TOTAL_SPENT,\n        ch.AVG_ITEM_SPEND,\n        ch.UNIQUE_PRODUCTS_BOUGHT,\n        ch.DAYS_SINCE_LAST_ORDER,\n        pp.TIMES_ORDERED as PRODUCT_POPULARITY,\n        pp.AVG_PRICE as PRODUCT_PRICE,\n        pp.TOTAL_QUANTITY_SOLD as PRODUCT_VOLUME,\n        COALESCE(pp.PRICE_VARIANCE, 0) as PRODUCT_PRICE_VARIANCE,\n        0 as PURCHASED\n    FROM customer_history ch\n    CROSS JOIN product_popularity pp\n    LEFT JOIN customer_products cp \n        ON ch.CUSTOMER_ID = cp.CUSTOMER_ID \n        AND pp.PRODUCT_ID = cp.PRODUCT_ID\n    WHERE cp.PRODUCT_ID IS NULL\n)\nSELECT * FROM positive_examples\nUNION ALL\nSELECT * FROM negative_examples;\n\n-- Check class balance\nSELECT \n    'Total Samples' as metric,\n    COUNT(*)::VARCHAR as value\nFROM training_features\nUNION ALL\nSELECT \n    'Class: ' || PURCHASED as metric,\n    COUNT(*)::VARCHAR as value\nFROM training_features \nGROUP BY PURCHASED\nORDER BY metric;"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "load-features-pandas",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_1"
      },
      "outputs": [],
      "source": "-- Preview features\nSELECT * FROM training_features LIMIT 10;"
    },
    {
      "cell_type": "markdown",
      "id": "prepare-training",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Load Data into Pandas for Training\n\nLoad training features from Snowflake table."
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "load-to-pandas",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Load data from Snowflake (imports already done at top)\nprint(\"ðŸ“Š Loading data from Snowflake...\")\nstart_load = time.time()\n\n# Load data from Snowflake table\npdf = session.sql(\"SELECT * FROM training_features\").to_pandas()\n\nload_time = time.time() - start_load\n\nprint(f\"\\nâœ… Data loaded in {load_time:.2f} seconds\")\nprint(f\"   Rows: {len(pdf):,}\")\nprint(f\"   Columns: {len(pdf.columns)}\")\n\n# Preview\nprint(\"\\nðŸ“‹ Data Preview:\")\npdf.head()"
    },
    {
      "cell_type": "markdown",
      "id": "encode-features",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. Encode Categorical Features"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "encoding",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Encode customer_id and product_id\ncustomer_encoder = LabelEncoder()\nproduct_encoder = LabelEncoder()\n\npdf['CUSTOMER_ID_ENCODED'] = customer_encoder.fit_transform(pdf['CUSTOMER_ID'])\npdf['PRODUCT_ID_ENCODED'] = product_encoder.fit_transform(pdf['PRODUCT_ID'])\n\n# Define features for training (added more features for complexity)\nfeature_columns = [\n    'CUSTOMER_ID_ENCODED',\n    'PRODUCT_ID_ENCODED',\n    'TOTAL_PAST_ORDERS',\n    'TOTAL_SPENT',\n    'AVG_ITEM_SPEND',\n    'UNIQUE_PRODUCTS_BOUGHT',\n    'DAYS_SINCE_LAST_ORDER',\n    'PRODUCT_POPULARITY',\n    'PRODUCT_PRICE',\n    'PRODUCT_VOLUME',\n    'PRODUCT_PRICE_VARIANCE'\n]\n\nlabel_column = 'PURCHASED'\n\n# Prepare X and y\nX = pdf[feature_columns]\ny = pdf[label_column]\n\nprint(f\"ðŸ“Š Features shape: {X.shape}\")\nprint(f\"   Training samples: {len(X):,}\")\nprint(f\"   Features: {len(feature_columns)}\")\nprint(f\"   Label distribution: {y.value_counts().to_dict()}\")\nprint(f\"\\nâœ… Features encoded successfully\")"
    },
    {
      "cell_type": "markdown",
      "id": "train-model",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Train XGBoost Model with GPU\n\n**GPU Training with XGBoost:**\n- `tree_method='gpu_hist'`: GPU-accelerated histogram-based training\n- `predictor='gpu_predictor'`: GPU-accelerated predictions\n- Significantly faster training on large datasets"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "split-data",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"ðŸ“Š Data Split:\")\nprint(f\"   Training set: {len(X_train):,} samples\")\nprint(f\"   Test set: {len(X_test):,} samples\")"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "train-xgboost",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Configure model parameters (imports already done at top)\n# High complexity parameters for GPU training\ncommon_params = {\n    'n_estimators': 1000,          # 1000 trees for substantial workload\n    'max_depth': 20,               # Deep trees for complexity\n    'learning_rate': 0.03,         # Lower learning rate for accuracy\n    'min_child_weight': 1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'gamma': 0.1,                  # Regularization\n    'reg_alpha': 0.1,              # L1 regularization\n    'reg_lambda': 1.0,             # L2 regularization\n    'random_state': 42,\n    'eval_metric': 'logloss'\n}\n\nprint(\"=\"*70)\nprint(\"ðŸš€ GPU-ACCELERATED ML TRAINING\")\nprint(\"=\"*70)\nprint(f\"\\nðŸ“Š Dataset:\")\nprint(f\"   Training samples: {len(X_train):,}\")\nprint(f\"   Test samples: {len(X_test):,}\")\nprint(f\"   Features: {len(feature_columns)}\")\nprint(f\"\\nðŸ“‹ Model Configuration:\")\nprint(f\"   Trees: {common_params['n_estimators']}\")\nprint(f\"   Max Depth: {common_params['max_depth']}\")\nprint(f\"   Learning Rate: {common_params['learning_rate']}\")\nprint(f\"   Regularization: L1={common_params['reg_alpha']}, L2={common_params['reg_lambda']}\")\nprint(\"\\n\" + \"=\"*70)\n\ngpu_params = {**common_params, **{\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor'\n}}\n\nmodel = xgb.XGBClassifier(**gpu_params)\n\nprint(f\"â³ GPU training started at {time.strftime('%H:%M:%S')}...\\n\")\nstart_time = time.time()\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=200  # Print every 200 trees\n)\n\ntraining_time = time.time() - start_time\n\nprint(f\"\\nâœ… GPU training complete at {time.strftime('%H:%M:%S')}!\")\nprint(f\"\\nâš¡ GPU Performance:\")\nprint(f\"   Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\nprint(f\"   Trees per second: {common_params['n_estimators']/training_time:.2f}\")\nprint(f\"   Samples per second: {len(X_train)/training_time:,.0f}\")\n\n# Evaluate model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nðŸŽ¯ Model Accuracy: {accuracy:.4f}\")"
    },
    {
      "cell_type": "markdown",
      "id": "evaluate",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 9. Evaluate Model Performance"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "predictions",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Make predictions\ny_pred = model.predict(X_test)\ny_pred_proba = model.predict_proba(X_test)[:, 1]\n\n# Classification report\nprint(\"ðŸ“Š Model Performance:\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Not Purchased', 'Purchased']))\n\n# Calculate accuracy metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\"\\nðŸ“ˆ Summary Metrics:\")\nprint(f\"   Accuracy: {accuracy:.4f}\")\nprint(f\"   Precision: {precision:.4f}\")\nprint(f\"   Recall: {recall:.4f}\")\nprint(f\"   F1 Score: {f1:.4f}\")\n\n# Business interpretation\nprint(f\"\\nðŸ’¼ Business Impact:\")\nprint(f\"   If we recommend products to customers:\")\nprint(f\"   - {precision*100:.1f}% of recommendations will be relevant\")\nprint(f\"   - We'll capture {recall*100:.1f}% of products customers want\")"
    },
    {
      "cell_type": "markdown",
      "id": "feature-importance",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 10. Feature Importance Analysis"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "importance",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Get feature importance\nfeature_importance = model.feature_importances_\n\n# Create DataFrame for visualization\nfeature_importance_df = pd.DataFrame({\n    'feature': feature_columns,\n    'importance': feature_importance\n}).sort_values('importance', ascending=False)\n\nprint(\"ðŸ” Feature Importance:\")\nprint(feature_importance_df.to_string(index=False))\n\nprint(\"\\nðŸ’¡ Key Insights:\")\ntop_3_features = feature_importance_df.head(3)\nfor idx, row in top_3_features.iterrows():\n    print(f\"   {row['feature']}: {row['importance']:.2%}\")"
    },
    {
      "cell_type": "markdown",
      "id": "save-model",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 11. Save Model to Snowflake Model Registry\n\n**Note:** Requires Snowflake ML package and appropriate permissions."
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "save-to-registry",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Initialize Model Registry (imports already done at top)\nregistry = Registry(\n    session=session,\n    database_name=\"AUTOMATED_INTELLIGENCE\",\n    schema_name=\"MODELS\"\n)\n\n# Prepare model metadata\nmodel_name = \"product_recommendation_xgboost\"\nversion_name = f\"v_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_spcs\"\n\n# Sample input for schema inference\nsample_input = X_test.head(5)\n\n# Save model\nprint(f\"ðŸ’¾ Saving model to registry...\")\nprint(f\"   Model: {model_name}\")\nprint(f\"   Version: {version_name}\")\n\ntry:\n    mv = registry.log_model(\n        model=model,\n        model_name=model_name,\n        version_name=version_name,\n        sample_input_data=sample_input,\n        target_platforms=[\"SNOWPARK_CONTAINER_SERVICES\"],\n        conda_dependencies=[\"xgboost\", \"scikit-learn\", \"pandas\", \"numpy\"],     \n        comment=f\"XGBoost product recommender. Trained on {len(X_train):,} customer-product pairs. F1: {f1:.4f}\",\n        metrics={\n            \"accuracy\": float(accuracy),\n            \"precision\": float(precision),\n            \"recall\": float(recall),\n            \"f1_score\": float(f1),\n            \"training_samples\": int(len(X_train)),\n            \"test_samples\": int(len(X_test)),\n            \"training_time_seconds\": float(training_time)\n        }\n    )\n\n    # Create SPCS service and deploy to system GPU pool\n    mv.create_service(\n        service_name=\"gpu_xgboost_service\",\n        service_compute_pool=\"SYSTEM_COMPUTE_POOL_GPU\",\n        ingress_enabled=True,\n        gpu_requests=\"1\"  # Enable GPU for gpu_predictor\n    )\n\n    print(f\"\\nâœ… Model saved successfully!\")\n    print(f\"\\nðŸ“¦ Model Details:\")\n    print(f\"   Database: AUTOMATED_INTELLIGENCE\")\n    print(f\"   Schema: MODELS\")\n    print(f\"   Model: {model_name}\")\n    print(f\"   Version: {version_name}\")\n    print(f\"   Training Time: {training_time:.2f}s\")\nexcept Exception as e:\n    print(f\"âš ï¸  Error saving model: {e}\")\n    print(\"   Check that AUTOMATED_INTELLIGENCE.MODELS schema exists and you have write permissions\")"
    },
    {
      "cell_type": "markdown",
      "id": "test-predictions",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 12. Test Model Predictions from Registry\n\nLoad the registered model and generate product recommendations for sample customers."
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "sample-predictions",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Load model from registry (reuse registry object from previous cell)\nprint(\"ðŸ“¦ Attempting to load model from Snowflake Model Registry...\")\n\ntry:\n    # Get all versions and sort by creation time to get the most recent\n    model_obj = registry.get_model('product_recommendation_xgboost')\n    versions = model_obj.show_versions()\n    \n    # Sort by created_on descending to get latest\n    versions = versions.sort_values('created_on', ascending=False)\n    \n    # Get the latest version (first after sorting)\n    if len(versions) > 0:\n        latest_version = versions.iloc[0]['name']\n        model_ref = model_obj.version(latest_version)\n        loaded_model = model_ref.load()\n        \n        print(f\"\\nâœ… Model loaded from registry: product_recommendation_xgboost\")\n        print(f\"   Version: {latest_version} (latest)\")\n        using_registry = True\n    else:\n        raise ValueError(\"No versions found\")\nexcept Exception as e:\n    print(f\"âš ï¸  Error loading from registry: {e}\")\n    print(\"   Using freshly trained model instead\")\n    loaded_model = model\n    using_registry = False\n\nprint(f\"\\nðŸ”® Generating predictions...\\n\")\n\n# Get predictions for test set\nX_sample = X_test.head(20)\npredictions_proba = loaded_model.predict_proba(X_sample)[:, 1]\n\n# Create results DataFrame\nresults = X_sample.copy()\nresults['PURCHASE_PROBABILITY'] = predictions_proba\nresults['PREDICTION'] = (predictions_proba > 0.5).astype(int)\nresults['ACTUAL'] = y_test.iloc[:20].values\n\n# Sort by probability\nresults = results.sort_values('PURCHASE_PROBABILITY', ascending=False)\n\nprint(\"ðŸ”® Top Product Recommendations:\")\nprint(results[['CUSTOMER_ID_ENCODED', 'PRODUCT_ID_ENCODED', \n               'PURCHASE_PROBABILITY', 'PREDICTION', 'ACTUAL']].to_string(index=False))\n\n# Calculate recommendation accuracy\ncorrect = (results['PREDICTION'] == results['ACTUAL']).sum()\ntotal = len(results)\nprint(f\"\\nâœ… Recommendation Accuracy: {correct}/{total} ({correct/total*100:.1f}%)\")\n\nif using_registry:\n    print(f\"\\nðŸ’¡ Predictions made using model from Model Registry!\")\nelse:\n    print(f\"\\nðŸ’¡ Predictions made using trained model (save to registry to test loading)\")"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}