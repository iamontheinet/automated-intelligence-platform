{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "lastEditStatus": {
   "notebookId": "drc6hylddbqhycjwz3og",
   "authorId": "51208522045",
   "authorName": "DASH",
   "authorEmail": "dash.desai@snowflake.com",
   "sessionId": "62d477ac-91d7-4510-ad4e-8114765a9448",
   "lastEditTime": 1765070346248
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Customer Churn Prediction with Ray on Snowflake\n",
    "\n",
    "This notebook demonstrates distributed ML model training using Ray on Snowflake.\n",
    "\n",
    "**What we'll do:**\n",
    "1. Connect to Ray cluster and scale it dynamically\n",
    "2. Load customer data from Interactive Tables\n",
    "3. Engineer features for churn prediction\n",
    "4. Train XGBoost model using Ray distributed training\n",
    "5. Evaluate model performance\n",
    "6. Save model to Snowflake Model Registry\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run this in a Snowflake Notebook with ML Runtime\n",
    "- Data should exist in AUTOMATED_INTELLIGENCE.INTERACTIVE.CUSTOMER_ORDER_ANALYTICS"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## 1. Setup: Connect to Ray Cluster"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell3",
    "language": "python"
   },
   "outputs": [],
   "source": "import ray\nfrom snowflake.ml.runtime_cluster import scale_cluster, get_nodes, get_ray_dashboard_url\nimport snowflake.snowpark as snowpark\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import Session\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom snowflake.ml.registry import Registry\nimport datetime\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n\n# Get Snowflake session (automatically available in Snowflake Notebooks)\nsession = snowpark.context.get_active_session()\n\n# Connect to Ray cluster\nray.init(address=\"auto\", ignore_reinit_error=True)\nprint(f\"Ray cluster resources: {ray.cluster_resources()}\")\nprint(f\"Current cluster size: {len(get_nodes())} nodes\")\n\n# Get Ray Dashboard URL\ndashboard_url = get_ray_dashboard_url()\nprint(f\"\\nðŸŽ¯ Ray Dashboard: {dashboard_url}\")\nprint(\"   Open this URL in a new tab to monitor your cluster\")",
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "## 2. Scale Ray Cluster\n",
    "\n",
    "For this demo, we'll scale to 4 nodes (1 head + 3 workers) to demonstrate distributed training."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Scale cluster to 4 nodes\n",
    "print(\"Scaling cluster to 4 nodes...\")\n",
    "scale_cluster(expected_cluster_size=4)\n",
    "print(f\"âœ… Cluster scaled to {len(get_nodes())} nodes\")\n",
    "print(f\"Updated resources: {ray.cluster_resources()}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "## 3. Load Data from Snowflake\n",
    "\n",
    "We'll load customer order analytics data and engineer features for churn prediction."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell7",
    "language": "python"
   },
   "outputs": [],
   "source": "# Load customer analytics data\ndf = session.table(\"AUTOMATED_INTELLIGENCE.INTERACTIVE.CUSTOMER_ORDER_ANALYTICS\")\n\n# Add derived columns\ndf = df.with_column('CUSTOMER_TENURE_DAYS',\n    F.datediff('day', F.col('FIRST_ORDER_DATE'), F.col('LAST_ORDER_DATE')))\n\ndf = df.with_column('DAYS_SINCE_LAST_ORDER',\n    F.datediff('day', F.col('LAST_ORDER_DATE'), F.current_date()))\n\n# Filter to customers with orders\ndf = df.filter(F.col('TOTAL_ORDERS') > 0)\n\nprint(f\"\\nðŸ“Š Loaded {df.count():,} customers\")\nprint(\"\\nSample data:\")\ndf.select('CUSTOMER_ID', 'TOTAL_ORDERS', 'TOTAL_SPENT', 'CUSTOMER_TENURE_DAYS', 'DAYS_SINCE_LAST_ORDER').show(5)",
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "## 4. Feature Engineering\n\nCreate features for churn prediction:\n- **Churn label**: Customer hasn't ordered in 7+ days (target variable)\n- **Frequency**: Total orders (historical behavior)\n- **Monetary**: Total spent, average order value (spending patterns)\n- **Tenure**: Days as customer (customer age)\n- **Order frequency**: Orders per day of tenure (engagement rate)\n\n**Important:** We exclude `days_since_last_order` to avoid data leakage - it directly reveals the target variable.\n\n**Note:** Using 7-day threshold for demo purposes to ensure balanced classes.",
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell9",
    "language": "python"
   },
   "outputs": [],
   "source": "# Define churn: customers who haven't ordered in 7+ days\nCHURN_THRESHOLD_DAYS = 7\n\n# Create churn label column\ndf = df.with_column('IS_CHURNED', \n    (F.col('DAYS_SINCE_LAST_ORDER') > CHURN_THRESHOLD_DAYS).cast('int'))\n\n# Additional derived features (excluding leaky features)\ndf = df.with_column('ORDER_FREQUENCY', \n    F.col('TOTAL_ORDERS') / (F.col('CUSTOMER_TENURE_DAYS') + 1))\n\ndf = df.with_column('REVENUE_PER_ORDER', \n    F.col('TOTAL_SPENT') / F.col('TOTAL_ORDERS'))\n\n# Select features for training (NO days_since_last_order - that's data leakage!)\nfeature_columns = [\n    'TOTAL_ORDERS',\n    'TOTAL_SPENT', \n    'AVG_ORDER_VALUE',\n    'CUSTOMER_TENURE_DAYS',\n    'ORDER_FREQUENCY',\n    'REVENUE_PER_ORDER'\n]\n\nlabel_column = 'IS_CHURNED'\n\n# Show dataset statistics\ntotal_count = df.count()\nchurn_stats = df.group_by('IS_CHURNED').count().collect()\n\nprint(f\"ðŸ“Š Dataset Statistics:\")\nprint(f\"   Total samples: {total_count:,}\")\nprint(f\"\\n   Class distribution:\")\nfor row in churn_stats:\n    status = \"Active\" if row['IS_CHURNED'] == 0 else \"Churned\"\n    count = row['COUNT']\n    pct = (count / total_count) * 100\n    print(f\"   {status} customers: {count:,} ({pct:.1f}%)\")\n\n# Calculate class imbalance ratio\nactive_count = [r['COUNT'] for r in churn_stats if r['IS_CHURNED'] == 0][0]\nchurned_count = [r['COUNT'] for r in churn_stats if r['IS_CHURNED'] == 1][0]\nclass_ratio = active_count / churned_count\nprint(f\"\\n   Class imbalance ratio: {class_ratio:.1f}:1\")",
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "## 5. Train Model with Ray\n",
    "\n",
    "We'll use Ray Data for distributed data processing and XGBoost for training."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "id": "2803c5cc-9e4c-4c6f-8c37-475e6d8d3609",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "# Convert to pandas for sklearn\npdf = df.select(feature_columns + [label_column]).to_pandas()\n\n# Split features and target\nX = pdf[feature_columns]\ny = pdf[label_column]\n\n# Train/test split (stratified to maintain class balance)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"\\nðŸ“Š Data Split:\")\nprint(f\"   Training set: {len(X_train):,} samples\")\nprint(f\"   Test set: {len(X_test):,} samples\")\n\n# Create XGBoost classifier with class imbalance handling\nxgb_model = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    random_state=42,\n    scale_pos_weight=class_ratio,  # Handle class imbalance\n    tree_method='hist',  # Efficient distributed training\n    n_jobs=-1  # Use all available cores\n)\n\n# Create pipeline with StandardScaler + XGBoost\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', xgb_model)\n])\n\n# Train pipeline (Ray handles distributed execution across cluster)\n# Monitor Ray cluster during training\nprint(\"\\nðŸ“Š Ray Cluster Status BEFORE Training:\")\nprint(f\"   Available CPUs: {ray.available_resources().get('CPU', 0)}\")\nprint(f\"   Available Memory: {ray.available_resources().get('memory', 0) / 1e9:.2f} GB\")\n\nprint(\"\\nâ³ Training XGBoost model with Ray distributed computing...\")\nprint(f\"   Using {len(get_nodes())} nodes in Ray cluster\")\nprint(f\"   Class imbalance ratio: {class_ratio:.1f}:1\")\n\npipeline.fit(X_train, y_train)\n\nprint(\"âœ… Training complete!\")\n\n# Check resource usage after training\nprint(\"\\nðŸ“Š Ray Cluster Status AFTER Training:\")\nprint(f\"   Available CPUs: {ray.available_resources().get('CPU', 0)}\")\nprint(f\"   Available Memory: {ray.available_resources().get('memory', 0) / 1e9:.2f} GB\")\nprint(f\"   Model: Pipeline(StandardScaler + XGBClassifier)\")\nprint(f\"   Features: {len(feature_columns)}\")\nprint(f\"   Training samples: {len(X_train):,}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": [
    "## 6. Evaluate Model Performance"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell13",
    "language": "python"
   },
   "outputs": [],
   "source": "# Make predictions using pipeline\ny_pred = pipeline.predict(X_test)\ny_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n\n# Calculate metrics\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\nprint(\"ðŸ“Š Model Performance:\")\nprint(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Active', 'Churned']))\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_test, y_pred)\nprint(f\"                  Predicted\")\nprint(f\"                Active  Churned\")\nprint(f\"Actual Active    {cm[0,0]:6d}  {cm[0,1]:6d}\")\nprint(f\"       Churned   {cm[1,0]:6d}  {cm[1,1]:6d}\")\n\n# Calculate recall for churned customers\nchurned_recall = cm[1,1] / (cm[1,0] + cm[1,1])\nchurned_caught = cm[1,1]\nchurned_total = cm[1,0] + cm[1,1]\nprint(f\"\\nðŸŽ¯ Business Impact:\")\nprint(f\"   Churned customers caught: {churned_caught}/{churned_total} ({churned_recall*100:.1f}%)\")\nprint(f\"   Missed churned customers: {cm[1,0]} ({(1-churned_recall)*100:.1f}%)\")",
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell14"
   },
   "source": [
    "## 7. Feature Importance"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell15",
    "language": "python"
   },
   "outputs": [],
   "source": "# Get feature importance from the XGBoost model in the pipeline\nxgb_model = pipeline.named_steps['classifier']\nfeature_importance = xgb_model.feature_importances_\n\n# Create DataFrame for visualization\nfeature_importance_df = pd.DataFrame({\n    'feature': feature_columns,\n    'importance': feature_importance\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nðŸ” Feature Importance:\")\nprint(feature_importance_df.to_string(index=False))\n\nprint(\"\\nðŸ’¡ Insights:\")\ntop_feature = feature_importance_df.iloc[0]['feature']\ntop_importance = feature_importance_df.iloc[0]['importance']\nprint(f\"   Most important feature: {top_feature} ({top_importance:.2%})\")\nprint(f\"   This feature has the strongest predictive power for customer churn\")",
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell16"
   },
   "source": [
    "## 8. Save Model to Snowflake Model Registry"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell17",
    "language": "python"
   },
   "outputs": [],
   "source": "# Create registry\nregistry = Registry(session=session, database_name=\"AUTOMATED_INTELLIGENCE\", schema_name=\"MODELS\")\n\n# Prepare model metadata\nmodel_name = \"customer_churn_predictor\"\nversion_name = f\"v_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\n# Sample input for schema inference (UNSCALED data)\nsample_input = X_test.head(5)\n\n# Save pipeline (already trained above)\nprint(f\"\\nðŸ’¾ Saving model to registry...\")\nprint(f\"   Model: {model_name}\")\nprint(f\"   Version: {version_name}\")\n\nmv = registry.log_model(\n    model=pipeline,\n    model_name=model_name,\n    version_name=version_name,\n    sample_input_data=sample_input,\n    comment=f\"XGBoost churn predictor (Ray) with preprocessing pipeline. Trained on {len(X_train):,} customers. ROC-AUC: {roc_auc:.4f}\",\n    metrics={\n        \"roc_auc\": float(roc_auc),\n        \"training_samples\": int(len(X_train)),\n        \"test_samples\": int(len(X_test)),\n        \"churned_recall\": float(churned_recall),\n        \"churn_rate\": float(y.mean())\n    }\n)\n\nprint(f\"âœ… Model saved successfully!\")\nprint(f\"\\nðŸ“¦ Model Details:\")\nprint(f\"   Database: AUTOMATED_INTELLIGENCE\")\nprint(f\"   Schema: MODELS\")\nprint(f\"   Model: {model_name}\")\nprint(f\"   Version: {version_name}\")\nprint(f\"\\nðŸŽ¯ Next Steps:\")\nprint(f\"   1. View model in Snowsight: Data > Models > {model_name}\")\nprint(f\"   2. Check the Streamlit dashboard ML Insights page\")\nprint(f\"   3. Use model for predictions on new data\")",
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell18"
   },
   "source": [
    "## 9. Test Model Predictions"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell19",
    "language": "python"
   },
   "outputs": [],
   "source": "# Get some sample customers to predict\nsample_customers_df = df.limit(10)\n\nsample_customers_df = df.limit(10)\n\n# Convert to pandas using the SAME feature_columns list\nsample_pdf_full = sample_customers_df.select(\n    feature_columns + [label_column, 'CUSTOMER_ID', 'DAYS_SINCE_LAST_ORDER']\n).to_pandas()\n\n# Extract just the features in the exact same way as X_train was created\nX_sample = sample_pdf_full[feature_columns]\n\n# Use the pipeline for predictions\npredictions = pipeline.predict_proba(X_sample)[:, 1]\n\n# Display results\nresults = pd.DataFrame({\n    'customer_id': sample_pdf_full['CUSTOMER_ID'],\n    'total_orders': sample_pdf_full['TOTAL_ORDERS'],\n    'days_since_last_order': sample_pdf_full['DAYS_SINCE_LAST_ORDER'],\n    'actual_churned': sample_pdf_full[label_column],\n    'churn_probability': predictions,\n    'prediction': (predictions > 0.5).astype(int)\n})\n\nprint(\"\\nðŸ”® Sample Predictions:\")\nprint(results.to_string(index=False))\nprint(f\"\\nâœ… Tested on {len(results)} sample customers\")",
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell20"
   },
   "source": [
    "## 10. Cleanup: Scale Down Cluster"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell21",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Scale back to 1 node when done\n",
    "print(\"Scaling cluster back to 1 node...\")\n",
    "scale_cluster(expected_cluster_size=1)\n",
    "print(f\"âœ… Cluster scaled down to {len(get_nodes())} node\")\n",
    "print(\"\\nðŸŽ‰ Training complete! Model is ready for use in the Streamlit dashboard.\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000020"
  }
 ]
}